from typing import List, Literal, Sequence, TypedDict, Dict, Any, Callable, Optional
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END, START
from langgraph.types import Command
import asyncio
from IPython.display import Image, display
from PIL import Image as PILImage
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
import json
import time


# å®šç¾©éœ€æ±‚é …ç›®çš„çµæ§‹
class NeedItem(BaseModel):
    need: str = Field(description="éœ€æ±‚çš„åç¨±æˆ–æ¨™é¡Œ")
    summary: str = Field(description="éœ€æ±‚çš„ç°¡è¦ç¸½çµ")
    medical_insights: str = Field(description="é†«ç™‚å°ˆå®¶çš„æ´å¯Ÿå’Œå»ºè­°")
    tech_insights: str = Field(description="æŠ€è¡“å°ˆå®¶çš„æ´å¯Ÿå’Œè§£æ±ºæ–¹æ¡ˆ")
    strategy: str = Field(description="ç¶œåˆå¯¦æ–½ç­–ç•¥")

# å®šç¾©æ•´é«”éœ€æ±‚è¼¸å‡ºçµæ§‹
class NeedsOutput(BaseModel):
    needs: List[NeedItem] = Field(description="è­˜åˆ¥å‡ºçš„éœ€æ±‚åˆ—è¡¨")

# å®šç¾©ç‹€æ…‹çµæ§‹
class ReflectionState(TypedDict):
    messages: List[BaseMessage]
    medical_insights: List[str]
    engineering_insights: List[str]
    discussion_round: int
    max_rounds: int
    final_summary: str

# å®šç¾©ç‹€æ…‹æ›´æ–°å›èª¿é¡å‹
StatusCallback = Callable[[str, str, Dict[str, Any]], None]

# åˆå§‹åŒ– LLM
llm = ChatOpenAI(
    model="gpt-4.1-mini", 
    temperature=0.7)

class MedicalReflectionSystem:
    def __init__(self, max_discussion_rounds: int = 5, status_callback: Optional[StatusCallback] = None):
        self.max_rounds = max_discussion_rounds
        self.status_callback = status_callback
        self.graph = self._build_graph()
        self.checkpointer = MemorySaver()
        
        # åˆå§‹åŒ– parser
        self.parser = PydanticOutputParser(pydantic_object=NeedsOutput)
    
    def _emit_status(self, event_type: str, agent: str, data: Dict[str, Any]):
        """ç™¼é€ç‹€æ…‹æ›´æ–°"""
        if self.status_callback:
            self.status_callback(event_type, agent, data)

    def _build_graph(self):
        """å»ºç«‹ LangGraph å·¥ä½œæµç¨‹"""
        builder = StateGraph(ReflectionState)
        
        # æ·»åŠ ç¯€é»
        builder.add_node("medical_staff_agent", self.medical_staff_node)
        builder.add_node("engineer_agent", self.engineer_node)
        builder.add_node("collector", self.collector_node)
        
        # è¨­å®šèµ·å§‹é»
        builder.add_edge(START, "medical_staff_agent")
        
        # æ·»åŠ æ¢ä»¶é‚Š
        builder.add_conditional_edges(
            "medical_staff_agent",
            self._should_continue_discussion,
            {
                "medical": "medical_staff_agent",
                "engineer_agent": "engineer_agent",
                "collector": "collector",
                "end": END
            }
        )
        
        builder.add_conditional_edges(
            "engineer_agent", 
            self._should_continue_discussion,
            {
                "medical": "medical_staff_agent",
                "engineer_agent": "engineer_agent",
                "collector": "collector",
                "end": END
            }
        )
        
        builder.add_edge("collector", END)

        
        return builder.compile(checkpointer=self.checkpointer)
    
    def get_current_state(self, thread_id: str = "default"):
        """ç²å–ç•¶å‰ graph ç‹€æ…‹"""
        config = {"configurable": {"thread_id": thread_id}}
        return self.graph.get_state(config)
    
    def get_discussion_progress(self, thread_id: str = "default"):
        """ç²å–è¨è«–é€²åº¦å’Œå…§å®¹"""
        state = self.get_current_state(thread_id)
        
        if state and state.values:
            return {
                "current_round": state.values.get("discussion_round", 0),
                "max_rounds": state.values.get("max_rounds", self.max_rounds),
                "medical_insights": state.values.get("medical_insights", []),
                "engineering_insights": state.values.get("engineering_insights", []),
                "messages": state.values.get("messages", []),
                "final_summary": state.values.get("final_summary", "")
            }
        return None

    
    def medical_staff_node(self, state: ReflectionState) -> ReflectionState:
        """é†«ç™‚å°ˆå®¶ Agent"""
        # ç™¼é€æ€è€ƒé–‹å§‹ç‹€æ…‹
        self._emit_status("thinking_started", "medical_expert", {
            "round": state["discussion_round"] + 1,
            "message": "é†«ç™‚å°ˆå®¶æ­£åœ¨åˆ†æé†«ç™‚éœ€æ±‚å’Œæµç¨‹å•é¡Œ..."
        })
        
        medical_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä½è³‡æ·±çš„é†«ç™‚å°ˆå®¶ï¼Œå°ˆç²¾æ–¼é†«ç™‚ç³»çµ±ç®¡ç†å’Œè³‡æºé…ç½®ã€‚
            ä½ æ­£åœ¨èˆ‡å·¥ç¨‹å¸«è¨è«–é†«ç™‚è³‡æºå£…å¡çš„å•é¡Œã€‚è«‹å¾é†«ç™‚å°ˆæ¥­è§’åº¦åˆ†æå•é¡Œï¼Œ
            ä¸¦æå‡ºå…·é«”çš„é†«ç™‚éœ€æ±‚å’Œè§£æ±ºæ–¹æ¡ˆã€‚
            
            è¨è«–è¦å‰‡ï¼š
            1. å°ˆæ³¨æ–¼é†«ç™‚æµç¨‹ã€äººåŠ›é…ç½®ã€è¨­å‚™ç®¡ç†ç­‰é†«ç™‚å°ˆæ¥­é ˜åŸŸ
            2. èˆ‡å·¥ç¨‹å¸«é€²è¡Œå»ºè¨­æ€§å°è©±ï¼Œäº’ç›¸è£œå……è§€é»
            3. æå‡ºå…·é«”å¯è¡Œçš„é†«ç™‚æ”¹å–„å»ºè­°
            4. å›æ‡‰è¦ç°¡æ½”æ˜ç¢ºï¼Œé‡é»çªå‡º"""),
            MessagesPlaceholder(variable_name="messages")
        ])
        
        chain = medical_prompt | llm
        response = chain.invoke({"messages": state["messages"]})
        print("\n==========medical think... ==========\n ",response.content)
        
        # æ›´æ–°ç‹€æ…‹
        new_messages = state["messages"] + [response]
        medical_insights = state["medical_insights"] + [response.content]
        
        # ç™¼é€æ€è€ƒå®Œæˆç‹€æ…‹
        self._emit_status("thinking_completed", "medical_expert", {
            "round": state["discussion_round"] + 1,
            "response": response.content,
            "insight_count": len(medical_insights)
        })
        
        return {
            **state,
            "messages": new_messages,
            "medical_insights": medical_insights,
            "discussion_round": state["discussion_round"] + 1
        }
    
    def engineer_node(self, state: ReflectionState) -> ReflectionState:
        """å·¥ç¨‹å¸« Agent"""
        # ç™¼é€æ€è€ƒé–‹å§‹ç‹€æ…‹
        self._emit_status("thinking_started", "engineer", {
            "round": state["discussion_round"] + 1,
            "message": "å·¥ç¨‹å¸«æ­£åœ¨åˆ†ææŠ€è¡“è§£æ±ºæ–¹æ¡ˆå’Œç³»çµ±å„ªåŒ–..."
        })
        
        engineer_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä½è³‡æ·±çš„ç³»çµ±å·¥ç¨‹å¸«ï¼Œå°ˆç²¾æ–¼é†«ç™‚è³‡è¨Šç³»çµ±ã€æµç¨‹å„ªåŒ–å’ŒæŠ€è¡“è§£æ±ºæ–¹æ¡ˆã€‚
            ä½ æ­£åœ¨èˆ‡é†«ç™‚å°ˆå®¶è¨è«–é†«ç™‚è³‡æºå£…å¡çš„å•é¡Œã€‚è«‹å¾æŠ€è¡“å’Œç³»çµ±è§’åº¦åˆ†æå•é¡Œï¼Œ
            ä¸¦æå‡ºå…·é«”çš„æŠ€è¡“éœ€æ±‚å’Œè§£æ±ºæ–¹æ¡ˆã€‚
            
            è¨è«–è¦å‰‡ï¼š
            1. å°ˆæ³¨æ–¼ç³»çµ±æ¶æ§‹ã€æ•¸æ“šåˆ†æã€è‡ªå‹•åŒ–æµç¨‹ç­‰æŠ€è¡“é ˜åŸŸ
            2. èˆ‡é†«ç™‚å°ˆå®¶é€²è¡Œå»ºè¨­æ€§å°è©±ï¼Œç†è§£é†«ç™‚éœ€æ±‚ä¸¦æä¾›æŠ€è¡“æ”¯æ´
            3. æå‡ºå…·é«”å¯è¡Œçš„æŠ€è¡“æ”¹å–„å»ºè­°
            4. å›æ‡‰è¦ç°¡æ½”æ˜ç¢ºï¼Œé‡é»çªå‡º"""),
            MessagesPlaceholder(variable_name="messages")
        ])
        
        chain = engineer_prompt | llm
        response = chain.invoke({"messages": state["messages"]})
        print("\n==========engineer think... ==========\n ",response.content)
        
        # æ›´æ–°ç‹€æ…‹
        new_messages = state["messages"] + [response]
        engineering_insights = state["engineering_insights"] + [response.content]
        
        # ç™¼é€æ€è€ƒå®Œæˆç‹€æ…‹
        self._emit_status("thinking_completed", "engineer", {
            "round": state["discussion_round"] + 1,
            "response": response.content,
            "insight_count": len(engineering_insights)
        })
        
        return {
            **state,
            "messages": new_messages,
            "engineering_insights": engineering_insights,
            "discussion_round": state["discussion_round"] + 1
        }
    
    def collector_node(self, state: ReflectionState) -> ReflectionState:
        """æ”¶é›†è€… Agent - çµ±æ•´å„æ–¹éœ€æ±‚"""
        collector_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä½å°ˆæ¡ˆå”èª¿è€…ï¼Œè² è²¬çµ±æ•´é†«ç™‚å°ˆå®¶å’Œå·¥ç¨‹å¸«çš„è¨è«–çµæœã€‚
            è«‹åˆ†ææ•´å€‹å°è©±éç¨‹ï¼Œæå–é—œéµæ´å¯Ÿï¼Œä¸¦è­˜åˆ¥å‡ºå…·é«”çš„éœ€æ±‚é …ç›®ã€‚
            
            ä»»å‹™ï¼š
            1. å¾è¨è«–ä¸­è­˜åˆ¥å‡ºä¸åŒçš„éœ€æ±‚é …ç›®ï¼ˆå¯èƒ½æœ‰å¤šå€‹ï¼‰
            2. ç‚ºæ¯å€‹éœ€æ±‚é …ç›®æä¾›ï¼š
               - need: éœ€æ±‚çš„åç¨±æˆ–æ¨™é¡Œ
               - summary: è©²éœ€æ±‚çš„ç°¡è¦ç¸½çµ
               - medical_insights: é†«ç™‚å°ˆå®¶å°æ­¤éœ€æ±‚çš„æ´å¯Ÿå’Œå»ºè­°
               - tech_insights: å·¥ç¨‹å¸«å°æ­¤éœ€æ±‚çš„æŠ€è¡“è§£æ±ºæ–¹æ¡ˆ
               - strategy: é‡å°æ­¤éœ€æ±‚çš„ç¶œåˆå¯¦æ–½ç­–ç•¥
            3. æ¯å€‹éœ€æ±‚éƒ½æ‡‰è©²æ˜¯ç¨ç«‹ä¸”å…·é«”çš„
            4. è¼¸å‡ºæ ¼å¼å¿…é ˆæ˜¯ä¸€å€‹åŒ…å«éœ€æ±‚é …ç›®çš„åˆ—è¡¨
            
            {format_instructions}
            """),
            ("human", f"""
            é†«ç™‚å°ˆå®¶æ´å¯Ÿï¼š
            {chr(10).join(state['medical_insights'])}
            
            å·¥ç¨‹å¸«æ´å¯Ÿï¼š
            {chr(10).join(state['engineering_insights'])}
            
            å®Œæ•´å°è©±è¨˜éŒ„ï¼š
            {chr(10).join([msg.content for msg in state['messages'] if isinstance(msg, (AIMessage, HumanMessage))])}
            
            è«‹åˆ†æä¸¦è­˜åˆ¥å‡ºå…·é«”çš„éœ€æ±‚é …ç›®ï¼Œä»¥åˆ—è¡¨æ ¼å¼è¼¸å‡ºã€‚
            """)
        ])
        
        # æ ¼å¼åŒ– prompt åŒ…å« parser æŒ‡ç¤º
        formatted_prompt = collector_prompt.partial(
            format_instructions=self.parser.get_format_instructions()
        )
        
        chain = formatted_prompt | llm | self.parser
        
        try:
            response = chain.invoke({})
            
            # å°‡è§£æå¾Œçš„çµæœè½‰æ›ç‚ºå­—ç¬¦ä¸²ä»¥ä¾¿å­˜å„²
            parsed_output = response.model_dump()
            
        except Exception as e:
            print(f"è§£æéŒ¯èª¤: {e}")
            # å¦‚æœè§£æå¤±æ•—ï¼Œæä¾›é»˜èªçµæ§‹
            parsed_output = {
                "needs": [
                    {
                        "need": "è§£æå¤±æ•—çš„éœ€æ±‚",
                        "summary": "è§£æå¤±æ•—ï¼Œè«‹æª¢æŸ¥è¼¸å‡ºæ ¼å¼",
                        "medical_insights": "ç„¡æ³•è§£æé†«ç™‚æ´å¯Ÿ",
                        "tech_insights": "ç„¡æ³•è§£ææŠ€è¡“æ´å¯Ÿ",
                        "strategy": "ç„¡æ³•è§£æç­–ç•¥"
                    }
                ]
            }
        
        return {
            **state,
            "messages": state["messages"] + [AIMessage(content=str(parsed_output))],
            "final_summary": str(parsed_output)
        }

    def _should_continue_discussion(self, state: ReflectionState) -> str:
        """æ±ºå®šæ˜¯å¦ç¹¼çºŒè¨è«–"""
        current_round = state.get("discussion_round", 0)
        max_rounds = state.get("max_rounds", self.max_rounds)
        
        if current_round >= max_rounds:
            return "collector"
        
        # æª¢æŸ¥æœ€å¾Œä¸€æ¢è¨Šæ¯ä¾†æ±ºå®šä¸‹ä¸€å€‹ agent
        last_message = state["messages"][-1] if state["messages"] else None
        
        if last_message and isinstance(last_message, AIMessage):
            # æ ¹æ“šæœ€å¾Œä¸€æ¢è¨Šæ¯çš„ä¾†æºæ±ºå®šä¸‹ä¸€å€‹ agent
            judge_prompt = ChatPromptTemplate.from_messages([
                ("system", """ä½ æ˜¯ä¸€å€‹è¨è«–ä¸»é¡Œåˆ¤æ–·å™¨ã€‚è«‹åˆ†ææœ€è¿‘çš„å°è©±å…§å®¹ï¼Œåˆ¤æ–·è¨è«–çš„é‡é»æ˜¯åå‘é†«ç™‚å°ˆæ¥­é ˜åŸŸé‚„æ˜¯æŠ€è¡“å·¥ç¨‹é ˜åŸŸã€‚

                åˆ¤æ–·æ¨™æº–ï¼š
                - å¦‚æœè¨è«–é‡é»æ˜¯é†«ç™‚æµç¨‹ã€è‡¨åºŠç¶“é©—ã€ç—…æ‚£ç…§è­·ã€é†«ç™‚æ”¿ç­–ç­‰ï¼Œå›ç­” "medical"
                - å¦‚æœè¨è«–é‡é»æ˜¯æŠ€è¡“è§£æ±ºæ–¹æ¡ˆã€ç³»çµ±æ¶æ§‹ã€è»Ÿé«”é–‹ç™¼ã€æ•¸æ“šåˆ†æç­‰ï¼Œå›ç­” "engineering"
                
                è«‹åªå›ç­” "medical" æˆ– "engineering"ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ã€‚"""),
                ("human", f"æœ€è¿‘çš„å°è©±å…§å®¹ï¼š\n{last_message.content}")
            ])
            
            chain = judge_prompt | llm
            try:
                judgment = chain.invoke({}).content.strip().lower()
                print(f"\n==========topic judgment==========\n{judgment}")
                
                # æ ¹æ“šåˆ¤æ–·çµæœæ±ºå®šä¸‹ä¸€å€‹ agent
                if "medical" in judgment:
                    return "engineer_agent"  # å¦‚æœç•¶å‰æ˜¯é†«ç™‚ä¸»é¡Œï¼Œä¸‹ä¸€å€‹æ‡‰è©²æ˜¯å·¥ç¨‹å¸«
                elif "engineering" in judgment:
                    return "medical"  # å¦‚æœç•¶å‰æ˜¯å·¥ç¨‹ä¸»é¡Œï¼Œä¸‹ä¸€å€‹æ‡‰è©²æ˜¯é†«ç™‚å°ˆå®¶
                else:
                    # å¦‚æœåˆ¤æ–·ä¸æ˜ç¢ºï¼Œé è¨­äº¤æ›¿é€²è¡Œ
                    return "engineer_agent" if current_round % 2 == 0 else "medical"
            except Exception as e:
                print(f"åˆ¤æ–·éŒ¯èª¤: {e}")
                # å¦‚æœ LLM åˆ¤æ–·å¤±æ•—ï¼Œå›åˆ°ç°¡å–®çš„äº¤æ›¿é‚è¼¯
                return "engineer_agent" if current_round % 2 == 0 else "medical"
        else:
            # å¦‚æœæ˜¯äººé¡è¨Šæ¯ï¼Œé è¨­å¾é†«ç™‚å°ˆå®¶é–‹å§‹
            return "medical"
            
    
    async def run_reflection(self, user_query: str) -> dict:
        """åŸ·è¡Œå®Œæ•´çš„ reflection æµç¨‹"""
        initial_state = {
            "messages": [HumanMessage(content=user_query)],
            "medical_insights": [],
            "engineering_insights": [],
            "discussion_round": 0,
            "max_rounds": self.max_rounds,
            "final_summary": ""
        }
        
        # åŸ·è¡Œå·¥ä½œæµç¨‹
        result = await self.graph.ainvoke(initial_state)
        
        
        # å˜—è©¦è§£ææœ€çµ‚çµæœ
        try:
            import ast
            parsed_needs = ast.literal_eval(result["final_summary"])
        except:
            parsed_needs = {"needs": []}
        
        return {
            "original_query": user_query,
            "discussion_rounds": result["discussion_round"],
            "medical_insights": result["medical_insights"],
            "engineering_insights": result["engineering_insights"],
            "parsed_needs": parsed_needs,
            "final_summary": result["final_summary"],
            "full_conversation": [msg.content for msg in result["messages"]]
        }

# åŒæ­¥ç‰ˆæœ¬çš„åŸ·è¡Œå‡½æ•¸
def run_reflection_sync(user_query: str, max_rounds: int = 3) -> dict:
    """åŒæ­¥ç‰ˆæœ¬çš„ reflection åŸ·è¡Œ"""
    reflection_system = MedicalReflectionSystem(max_discussion_rounds=max_rounds)
    
    initial_state = {
        "messages": [HumanMessage(content=user_query)],
        "medical_insights": [],
        "engineering_insights": [],
        "discussion_round": 0,
        "max_rounds": max_rounds,
        "final_summary": ""
    }
    
    # åŒæ­¥åŸ·è¡Œ
    result = reflection_system.graph.invoke(initial_state)
    
    # å˜—è©¦è§£ææœ€çµ‚çµæœ
    try:
        import ast
        parsed_needs = ast.literal_eval(result["final_summary"])
    except:
        parsed_needs = {"needs": []}
    
    return {
        "original_query": user_query,
        "discussion_rounds": result["discussion_round"],
        "medical_insights": result["medical_insights"],
        "engineering_insights": result["engineering_insights"],
        "parsed_needs": parsed_needs,
        "final_summary": result["final_summary"],
        "full_conversation": [msg.content for msg in result["messages"]]
    }

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    # åˆå§‹åŒ–ç³»çµ±
    reflection_system = MedicalReflectionSystem(max_discussion_rounds=3)
    
    # ä½¿ç”¨è€…æŸ¥è©¢
    user_query = "ç‚ºä»€éº¼é†«ç™‚è³‡æºæœƒå£…å¡ï¼Ÿæœ‰ä»€éº¼è§£æ±ºæ–¹æ¡ˆå—ï¼Ÿ"
    
    print("ğŸ¥ å•Ÿå‹•é†«ç™‚è³‡æºå£…å¡åˆ†æç³»çµ±...")
    print(f"ğŸ“ ä½¿ç”¨è€…å•é¡Œï¼š{user_query}")
    print("=" * 60)
    
    # åŸ·è¡Œ reflection æµç¨‹
    result = await reflection_system.run_reflection(user_query)
    
    # è¼¸å‡ºçµæœ
    print("\nğŸ“Š **è¨è«–çµæœç¸½è¦½**")
    print(f"è¨è«–è¼ªæ•¸ï¼š{result['discussion_rounds']}")
    print(f"é†«ç™‚æ´å¯Ÿæ•¸é‡ï¼š{len(result['medical_insights'])}")
    print(f"å·¥ç¨‹æ´å¯Ÿæ•¸é‡ï¼š{len(result['engineering_insights'])}")
    
    print("\nğŸ“‹ **è§£æå¾Œçš„éœ€æ±‚é …ç›®åˆ—è¡¨**")
    parsed_needs = result.get('parsed_needs', {}).get('needs', [])
    for i, need_info in enumerate(parsed_needs, 1):
        print(f"\nğŸ¯ **éœ€æ±‚é …ç›® {i}**")
        print(f"ğŸ·ï¸ éœ€æ±‚åç¨±: {need_info.get('need', 'N/A')}")
        print(f"ğŸ“ æ‘˜è¦: {need_info.get('summary', 'N/A')}")
        print(f"ğŸ¥ é†«ç™‚æ´å¯Ÿ: {need_info.get('medical_insights', 'N/A')[:150]}...")
        print(f"âš™ï¸ æŠ€è¡“æ´å¯Ÿ: {need_info.get('tech_insights', 'N/A')[:150]}...")
        print(f"ğŸ¯ ç­–ç•¥: {need_info.get('strategy', 'N/A')[:150]}...")

# åŒæ­¥ç‰ˆæœ¬çš„åŸ·è¡Œå‡½æ•¸
def run_reflection_sync(user_query: str, max_rounds: int = 3) -> dict:
    """åŒæ­¥ç‰ˆæœ¬çš„ reflection åŸ·è¡Œ"""
    reflection_system = MedicalReflectionSystem(max_discussion_rounds=max_rounds)
    
    initial_state = {
        "messages": [HumanMessage(content=user_query)],
        "medical_insights": [],
        "engineering_insights": [],
        "discussion_round": 0,
        "max_rounds": max_rounds,
        "final_summary": ""
    }
    
    # åŒæ­¥åŸ·è¡Œ
    result = reflection_system.graph.invoke(initial_state)
    # print("\n==========final result==========\n", result)

    # å˜—è©¦è§£ææœ€çµ‚çµæœ
    try:
        import ast
        parsed_needs = ast.literal_eval(result["final_summary"])
    except:
        parsed_needs = {"needs": []}
    
    return {
        "original_query": user_query,
        "discussion_rounds": result["discussion_round"],
        "medical_insights": result["medical_insights"],
        "engineering_insights": result["engineering_insights"],
        "parsed_needs": parsed_needs,
        "final_summary": result["final_summary"],
        "full_conversation": [msg.content for msg in result["messages"]]
    }

if __name__ == "__main__":
    # ç•°æ­¥åŸ·è¡Œ
    # asyncio.run(main())
    from evaluator import NeedEvaluator
    evaluator = NeedEvaluator()

    while True:
        user_query = input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼ˆæˆ–è¼¸å…¥ 'exit' é€€å‡ºï¼‰ï¼š")
        if user_query.lower() == 'exit':
            print("é€€å‡ºç³»çµ±ã€‚")
            break
        result = run_reflection_sync(user_query)
        print(f"è¨è«–å›åˆæ•¸ï¼š{result['discussion_rounds']}")
        n = 1
        print("\nğŸ“‹ ==========è§£æå¾Œçš„éœ€æ±‚é …ç›®åˆ—è¡¨==========\n")
        for need in result['parsed_needs']['needs']:
            print(
                  
                  f"need {n}: {need['need']}\n"
                  f"æ‘˜è¦: {need['summary']}\n"
                  f"é†«ç™‚çš„è§€é»: {need['medical_insights']}\n"
                  f"æŠ€è¡“çš„è§€é»: {need['tech_insights']}\n"
                  f"å»ºè­°åŠç­–ç•¥: {need['strategy']}\n"
                  f"-------------------\n")
            n += 1
            time.sleep(0.5)  # æ¨¡æ“¬è¼¸å‡ºå»¶é²
        eval_reslt = evaluator.evaluate_needs(result['parsed_needs']['needs'])
        print("\nğŸ“Š ==========è©•ä¼°çµæœ==========\n"
              f"ç¸½åˆ†: {eval_reslt.total_score}\n"
              f"é†«ç™‚æ´å¯Ÿåˆ†æ•¸: {eval_reslt.medical_insights_score}\n"
              f"æŠ€è¡“æ´å¯Ÿåˆ†æ•¸: {eval_reslt.tech_insights_score}\n"
              f"å¯¦æ–½ç­–ç•¥åˆ†æ•¸: {eval_reslt.strategy_score}\n"
              f"å„ªå…ˆéœ€æ±‚: {', '.join(eval_reslt.top_priority_needs)}\n")
        n = 0
